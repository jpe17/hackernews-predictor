{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with Hyperparameter Optimization\n",
    "\n",
    "Unlock the power of word embeddings with our Word2Vec implementation using the CBOW algorithm! 🌊 This project is designed for easy use and optimization. Here’s how to get started:\n",
    "\n",
    "- **🚀 Quick Setup**: Clone the repository and install the required dependencies.\n",
    "- **📊 Data Preparation**: Prepare your text data for training by following the provided guidelines.\n",
    "- **🔧 Hyperparameter Tuning**: Adjust parameters like learning rate, batch size, and embedding dimensions to optimize performance.\n",
    "- **📈 Experiment Tracking**: Use Weights & Biases (WandB) to track your experiments and visualize results effortlessly.\n",
    "- **🎉 Run the Model**: Execute the training script to build and fine-tune your Word2Vec model.\n",
    "\n",
    "Join us on this exciting journey of building and optimizing a powerful Word2Vec model! ✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1**: Setting Up the Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EXAMPLES = 1000000\n",
    "MIN_LR = 0.01\n",
    "MAX_LR = 0.2\n",
    "MIN_BATCH_SIZE = 200\n",
    "MAX_BATCH_SIZE = 400\n",
    "MIN_EMBEDDING_DIM = 512\n",
    "MAX_EMBEDDING_DIM = 1024\n",
    "MIN_WINDOW_SIZE = 2\n",
    "MAX_WINDOW_SIZE = 4\n",
    "MIN_WEIGHT_DECAY = 1e-6\n",
    "MAX_WEIGHT_DECAY = 1e-4\n",
    "MIN_LOSS_THRESHOLD = 2.0\n",
    "MAX_EPOCHS = 10\n",
    "MAX_GRAD_NORM = 1.0 \n",
    "MOMENTUM = 0.9\n",
    "SWEEP_COUNT = 2\n",
    "LR_SCHEDULER_FACTOR = 0.5\n",
    "LR_SCHEDULER_PATIENCE = 2\n",
    "\n",
    "FOLDER_TO_CONTINUE_TRAINING = \"artifacts/word2vec_trained_model/...\" \n",
    "\n",
    "OPTIMIZERS = ['adamw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2**: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 17,005,207\n",
      "Vocabulary size: 71,290\n",
      "Training words: 16,718,844\n",
      "Sample words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "# Load and process text8 data\n",
    "import collections\n",
    "\n",
    "# Read text8 dataset\n",
    "with open('../data/text8', 'r') as f:\n",
    "    words = f.read().split()\n",
    "\n",
    "print(f\"Total words: {len(words):,}\")\n",
    "\n",
    "# Build vocabulary (words appearing at least 5 times)\n",
    "word_counts = collections.Counter(words)\n",
    "vocabulary = {word: count for word, count in word_counts.items() if count >= 5}\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary.keys())}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocabulary):,}\")\n",
    "\n",
    "# Convert to indices (subsample for training speed)\n",
    "indexed_words = [word_to_index[word] for word in words if word in vocabulary]\n",
    "\n",
    "print(f\"Training words: {len(indexed_words):,}\")\n",
    "print(f\"Sample words: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_training_data(indexed_words, window_size, max_examples=None):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "\n",
    "    window_size = int(window_size)\n",
    "    \n",
    "    if max_examples is None:\n",
    "        end_idx = len(indexed_words) - window_size\n",
    "    else:\n",
    "        end_idx = min(len(indexed_words) - window_size, max_examples + window_size)\n",
    "\n",
    "    for i in range(window_size, end_idx):\n",
    "        context = (indexed_words[i - window_size:i] +\n",
    "                   indexed_words[i + 1:i + window_size + 1])\n",
    "        target = indexed_words[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "\n",
    "    return torch.tensor(contexts), torch.tensor(targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3**: NN Model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model functions ready!\n"
     ]
    }
   ],
   "source": [
    "# Simple Word2Vec Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SimpleWord2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        # Good initialization\n",
    "        nn.init.xavier_uniform_(self.embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "    \n",
    "    def forward(self, context):\n",
    "        # Bag of words: average context embeddings\n",
    "        embedded = self.embeddings(context)\n",
    "        hidden = torch.tanh(torch.mean(embedded, dim=1))\n",
    "        output = self.linear(hidden)\n",
    "        return output\n",
    "\n",
    "print(\"✅ Model functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4**: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Sweep\n",
    "import wandb\n",
    "\n",
    "def log_uniform_param(min_value, max_value):\n",
    "    \"\"\"Helper function to create a log uniform parameter configuration.\"\"\"\n",
    "    return {\n",
    "        'distribution': 'log_uniform_values',\n",
    "        'min': min_value,\n",
    "        'max': max_value\n",
    "    }\n",
    "\n",
    "# Adjusted sweep configuration for large dataset (16.7M words)\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Smart parameter search\n",
    "    'metric': {'name': 'best_loss', 'goal': 'minimize'},\n",
    "    'parameters': {\n",
    "        'learning_rate': log_uniform_param(MIN_LR, MAX_LR),\n",
    "        'batch_size': log_uniform_param(MIN_BATCH_SIZE, MAX_BATCH_SIZE),\n",
    "        'embedding_dim': log_uniform_param(MIN_EMBEDDING_DIM, MAX_EMBEDDING_DIM),\n",
    "        'window_size': log_uniform_param(MIN_WINDOW_SIZE, MAX_WINDOW_SIZE),\n",
    "        'optimizer': {'values': OPTIMIZERS},  # Add AdamW for better regularization\n",
    "        'weight_decay': log_uniform_param(MIN_WEIGHT_DECAY, MAX_WEIGHT_DECAY)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Sweep configuration ready!\n",
      "Will test: ['learning_rate', 'batch_size', 'embedding_dim', 'window_size', 'optimizer', 'weight_decay']\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    \"\"\"Training function for wandb sweep\"\"\"\n",
    "    # Initialize wandb\n",
    "    run = wandb.init()\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Round parameters to the nearest integer\n",
    "    batch_size = int(round(config.batch_size))\n",
    "    embedding_dim = int(round(config.embedding_dim))\n",
    "    window_size = int(round(config.window_size))\n",
    "\n",
    "    print(f\"Testing: lr={config.learning_rate:.4f}, batch={batch_size}, \"\n",
    "          f\"embed={embedding_dim}, window={window_size}\")\n",
    "    \n",
    "    # Create training data\n",
    "    contexts, targets = create_training_data(\n",
    "        indexed_words, \n",
    "        window_size,\n",
    "        max_examples=MAX_EXAMPLES\n",
    "    )\n",
    "\n",
    "    print(f\"💪 Training with {len(contexts):,} examples (not all available data!)\")\n",
    "    \n",
    "    # Setup training\n",
    "    dataset = TensorDataset(contexts, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleWord2Vec(len(vocabulary), embedding_dim).to(device)\n",
    "    \n",
    "    # Choose optimizer\n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), \n",
    "                              lr=config.learning_rate, \n",
    "                              weight_decay=config.weight_decay)\n",
    "    elif config.optimizer == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), \n",
    "                               lr=config.learning_rate, \n",
    "                               weight_decay=config.weight_decay)\n",
    "    else:  # sgd\n",
    "        optimizer = optim.SGD(model.parameters(), \n",
    "                             lr=config.learning_rate,\n",
    "                             momentum=MOMENTUM, \n",
    "                             weight_decay=config.weight_decay)\n",
    "        \n",
    "    # Initialize ReduceLROnPlateau scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                           mode='min', \n",
    "                                                           factor=LR_SCHEDULER_FACTOR, \n",
    "                                                           patience=LR_SCHEDULER_PATIENCE, \n",
    "                                                           verbose=True)\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    first_below_threshold_epoch = None  # Initialize the variable here\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_contexts, batch_targets in dataloader:\n",
    "            batch_contexts = batch_contexts.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_contexts)\n",
    "            loss = loss_function(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        best_loss = min(best_loss, avg_loss)\n",
    "        \n",
    "        # Log metrics\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'avg_loss': avg_loss,\n",
    "            'best_loss': best_loss\n",
    "        })\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Check if avg_loss falls below the threshold\n",
    "        if avg_loss < MIN_LOSS_THRESHOLD:\n",
    "            if first_below_threshold_epoch is None:  # Log the first epoch it falls below\n",
    "                first_below_threshold_epoch = epoch\n",
    "        \n",
    "        # Early stopping if converging well\n",
    "        if avg_loss < MIN_LOSS_THRESHOLD:\n",
    "            break\n",
    "    \n",
    "    # Log the first epoch below the threshold\n",
    "    if first_below_threshold_epoch is not None:\n",
    "        wandb.log({'first_below_threshold_epoch': first_below_threshold_epoch})\n",
    "    \n",
    "    # Final result\n",
    "    wandb.log({'final_best_loss': best_loss})\n",
    "    run.finish()\n",
    "    \n",
    "    return best_loss\n",
    "\n",
    "print(\"🎯 Sweep configuration ready!\")\n",
    "print(f\"Will test: {list(sweep_config['parameters'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/vscode/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoaopaesteves99\u001b[0m (\u001b[33mjoaopaesteves99-opensc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting hyperparameter sweep...\n",
      "Create sweep with ID: a7zt787a\n",
      "Sweep URL: https://wandb.ai/joaopaesteves99-opensc/word2vec/sweeps/a7zt787a\n",
      "📊 Sweep ID: a7zt787a\n",
      "🌐 Monitor at: https://wandb.ai/joaopaesteves99-opensc/word2vec/sweeps/a7zt787a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g7xw85nw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 305.9923764978384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 1006.9338913366272\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.03377521654937487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 4.292177166716253e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twindow_size: 2.8462799369857383\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20250615_170323-g7xw85nw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaopaesteves99-opensc/word2vec/runs/g7xw85nw' target=\"_blank\">eternal-sweep-1</a></strong> to <a href='https://wandb.ai/joaopaesteves99-opensc/word2vec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joaopaesteves99-opensc/word2vec/sweeps/a7zt787a' target=\"_blank\">https://wandb.ai/joaopaesteves99-opensc/word2vec/sweeps/a7zt787a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaopaesteves99-opensc/word2vec' target=\"_blank\">https://wandb.ai/joaopaesteves99-opensc/word2vec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joaopaesteves99-opensc/word2vec/sweeps/a7zt787a' target=\"_blank\">https://wandb.ai/joaopaesteves99-opensc/word2vec/sweeps/a7zt787a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaopaesteves99-opensc/word2vec/runs/g7xw85nw' target=\"_blank\">https://wandb.ai/joaopaesteves99-opensc/word2vec/runs/g7xw85nw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: lr=0.0338, batch=306, embed=1007, window=3\n",
      "💪 Training with 10,000,000 examples (not all available data!)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "wandb.login(key=wandb_api_key)\n",
    "\n",
    "# Run the hyperparameter sweep\n",
    "print(\"🚀 Starting hyperparameter sweep...\")\n",
    "\n",
    "# Create and run sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"word2vec\")\n",
    "print(f\"📊 Sweep ID: {sweep_id}\")\n",
    "print(f\"🌐 Monitor at: https://wandb.ai/{wandb.api.default_entity}/word2vec/sweeps/{sweep_id}\")\n",
    "\n",
    "# Run 8 experiments\n",
    "wandb.agent(sweep_id, train_model, count=SWEEP_COUNT)\n",
    "\n",
    "print(\"✅ Sweep complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters from local wandb runs (works without internet)\n",
    "def get_best_parameters_and_model_from_local(sweep_id=None):\n",
    "\n",
    "    import os\n",
    "    import json\n",
    "    import yaml\n",
    "    import glob\n",
    "    \n",
    "    # Find all local wandb runs\n",
    "    wandb_dir = \"wandb\"\n",
    "    run_dirs = glob.glob(os.path.join(wandb_dir, \"run-*\"))\n",
    "    \n",
    "    if sweep_id:\n",
    "        print(f\"🎯 Looking for best run with sweep/run ID filter: {sweep_id}\")\n",
    "        print(f\"📊 Found {len(run_dirs)} total local wandb runs...\")\n",
    "    else:\n",
    "        print(f\"🔍 Analyzing ALL local wandb runs...\")\n",
    "        print(f\"📊 Found {len(run_dirs)} local wandb runs...\")\n",
    "    \n",
    "    best_run_info = None\n",
    "    best_loss = float('inf')\n",
    "    best_run_dir = None\n",
    "    matching_runs = []\n",
    "    filtered_runs = []\n",
    "    \n",
    "    for run_dir in run_dirs:\n",
    "        try:\n",
    "            run_name = os.path.basename(run_dir)\n",
    "            \n",
    "            # If sweep_id is specified, filter runs\n",
    "            if sweep_id:\n",
    "                # Check if sweep_id matches either the run suffix or is contained in the run name\n",
    "                if sweep_id not in run_name:\n",
    "                    continue\n",
    "                filtered_runs.append(run_name)\n",
    "            \n",
    "            # Read the summary file to get final metrics\n",
    "            summary_file = os.path.join(run_dir, \"files\", \"wandb-summary.json\")\n",
    "            if os.path.exists(summary_file):\n",
    "                with open(summary_file, 'r') as f:\n",
    "                    summary = json.load(f)\n",
    "                \n",
    "                # Get the best loss from this run\n",
    "                loss = summary.get('best_loss', float('inf'))\n",
    "                \n",
    "                sweep_indicator = f\" ✅\" if sweep_id else \"\"\n",
    "                print(f\"  📋 {run_name}: loss = {loss:.4f}{sweep_indicator}\")\n",
    "                \n",
    "                matching_runs.append((run_name, loss))\n",
    "                \n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_run_dir = run_dir\n",
    "                    best_run_info = summary\n",
    "                    \n",
    "        except Exception as e:\n",
    "            run_name = os.path.basename(run_dir)\n",
    "            print(f\"  ⚠️  Could not read run {run_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if best_run_info and best_run_dir:\n",
    "        run_name = os.path.basename(best_run_dir)\n",
    "        if sweep_id:\n",
    "            print(f\"\\n🏆 BEST RUN FOUND (filtered by '{sweep_id}'): {run_name} (Loss: {best_loss:.4f})\")\n",
    "            print(f\"   📊 Matching runs: {len(matching_runs)} out of {len(run_dirs)} total\")\n",
    "        else:\n",
    "            print(f\"\\n🏆 BEST RUN FOUND (from all runs): {run_name} (Loss: {best_loss:.4f})\")\n",
    "        print(f\"   📁 Run directory: {best_run_dir}\")\n",
    "        \n",
    "        # Read the config file\n",
    "        config_file = os.path.join(best_run_dir, \"files\", \"config.yaml\")\n",
    "        if os.path.exists(config_file):\n",
    "            with open(config_file, 'r') as f:\n",
    "                config_data = yaml.safe_load(f)\n",
    "            \n",
    "            # Extract the actual config values\n",
    "            best_config = {}\n",
    "            for key, value_info in config_data.items():\n",
    "                if key.startswith('_'):  # Skip internal wandb keys\n",
    "                    continue\n",
    "                if isinstance(value_info, dict) and 'value' in value_info:\n",
    "                    best_config[key] = value_info['value']\n",
    "                else:\n",
    "                    best_config[key] = value_info\n",
    "            \n",
    "            # Round integer parameters\n",
    "            best_config['batch_size'] = int(round(best_config['batch_size']))\n",
    "            best_config['embedding_dim'] = int(round(best_config['embedding_dim']))\n",
    "            best_config['window_size'] = int(round(best_config['window_size']))\n",
    "            \n",
    "            print(f\"📋 Best Config:\")\n",
    "            for key, value in best_config.items():\n",
    "                print(f\"   - {key}: {value}\")\n",
    "            \n",
    "            # Check if we reached the threshold\n",
    "            first_below_threshold = best_run_info.get('first_below_threshold_epoch', None)\n",
    "            if first_below_threshold is not None:\n",
    "                print(f\"   ✅ Reached <{MIN_LOSS_THRESHOLD} at Epoch: {first_below_threshold}\")\n",
    "            \n",
    "            print(f\"\\n🔄 Creating model with best parameters...\")\n",
    "            \n",
    "            # Create model with best parameters\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model = SimpleWord2Vec(len(vocabulary), best_config['embedding_dim']).to(device)\n",
    "            \n",
    "            print(f\"✅ Model created with best parameters on {device}\")\n",
    "            print(f\"   - Embedding dim: {best_config['embedding_dim']}\")\n",
    "            print(f\"   - Vocab size: {len(vocabulary):,}\")\n",
    "            print(f\"   - Final loss: {best_loss:.4f}\")\n",
    "            \n",
    "            # Create a mock run object with the necessary info\n",
    "            class MockRun:\n",
    "                def __init__(self, run_dir, summary):\n",
    "                    self.name = os.path.basename(run_dir)\n",
    "                    self.summary = summary\n",
    "            \n",
    "            mock_run = MockRun(best_run_dir, best_run_info)\n",
    "            \n",
    "            return best_config, model, mock_run\n",
    "        else:\n",
    "            print(f\"❌ Config file not found: {config_file}\")\n",
    "            return None, None, None\n",
    "    else:\n",
    "        print(\"❌ No successful runs found in local wandb directory!\")\n",
    "        print(f\"   Checked directory: {wandb_dir}\")\n",
    "        print(f\"   Found {len(run_dirs)} run directories\")\n",
    "        return None, None, None\n",
    "\n",
    "# Get best parameters and model from local wandb runs\n",
    "# You can optionally specify a sweep_id or run identifier to filter results:\n",
    "# best_config, best_model, best_run = get_best_parameters_and_model_from_local(\"vtjlni2h\")  # specific sweep\n",
    "# best_config, best_model, best_run = get_best_parameters_and_model_from_local(\"gbvymzcf\")  # specific run suffix\n",
    "print(\"🔍 Searching for best model in local wandb runs...\")\n",
    "best_config, best_model, best_run = get_best_parameters_and_model_from_local()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the one to store\n",
    "best_config, best_model, best_run = get_best_parameters_and_model_from_local('run-20250615_163750-lo5zpuyu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 5**: Save Best Model and Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use the best model from hyperparameter sweep and run analysis\n",
    "if best_config and best_model:\n",
    "    print(\"🎯 Using best model from hyperparameter sweep for analysis...\")\n",
    "    \n",
    "    # Get embeddings from the best model (this is from the sweep, already trained)\n",
    "    embeddings = best_model.embeddings.weight.detach().cpu().numpy()\n",
    "    print(f\"📊 Embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Extract training information from the best run\n",
    "    final_loss = best_run.summary.get('best_loss', 'Unknown')\n",
    "    total_epochs = best_run.summary.get('epoch', 'Unknown') + 1 if best_run.summary.get('epoch') is not None else 'Unknown'\n",
    "    \n",
    "    print(f\"📈 Best model loss: {final_loss}\")\n",
    "    print(f\"📈 Training epochs: {total_epochs}\")\n",
    "    \n",
    "    # Word similarity analysis function\n",
    "    def find_similar_words(word, embeddings, word_to_index, index_to_word, top_k=5):\n",
    "        \"\"\"Find similar words using cosine similarity\"\"\"\n",
    "        if word not in word_to_index:\n",
    "            return f\"'{word}' not in vocabulary\"\n",
    "        \n",
    "        word_idx = word_to_index[word]\n",
    "        word_embed = embeddings[word_idx]\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarities = np.dot(embeddings, word_embed) / (\n",
    "            np.linalg.norm(embeddings, axis=1) * np.linalg.norm(word_embed)\n",
    "        )\n",
    "        \n",
    "        # Get top similar words\n",
    "        top_indices = np.argsort(similarities)[::-1][1:top_k+1]  # Skip the word itself\n",
    "        \n",
    "        similar_words = [(index_to_word[idx], similarities[idx]) \n",
    "                        for idx in top_indices]\n",
    "        return similar_words\n",
    "    \n",
    "    # Comprehensive word similarity test\n",
    "    test_categories = {\n",
    "        'Animals': ['dog', 'cat', 'bird', 'fish', 'horse'],\n",
    "        'Countries': ['america', 'england', 'france', 'china', 'japan'],\n",
    "        'Numbers': ['one', 'two', 'three', 'four', 'five'],\n",
    "        'Colors': ['red', 'blue', 'green', 'yellow', 'black'],\n",
    "        'Technology': ['computer', 'internet', 'software', 'technology'],\n",
    "        'Emotions': ['happy', 'sad', 'love', 'angry', 'fear']\n",
    "    }\n",
    "    \n",
    "    print(\"\\n🔍 COMPREHENSIVE WORD SIMILARITY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_similarity_results = {}\n",
    "    \n",
    "    for category, words in test_categories.items():\n",
    "        print(f\"\\n📋 {category.upper()}:\")\n",
    "        category_results = {}\n",
    "        \n",
    "        for word in words:\n",
    "            similar = find_similar_words(word, embeddings, word_to_index, index_to_word, 5)\n",
    "            if isinstance(similar, str):\n",
    "                print(f\"  {word}: {similar}\")\n",
    "                category_results[word] = similar\n",
    "            else:\n",
    "                similar_str = ', '.join([f\"{w}({s:.3f})\" for w, s in similar])\n",
    "                print(f\"  {word}: {similar_str}\")\n",
    "                category_results[word] = similar_str\n",
    "        \n",
    "        all_similarity_results[category] = category_results\n",
    "    \n",
    "    # Analogy testing (if possible)\n",
    "    def word_analogy(word1, word2, word3, embeddings, word_to_index, index_to_word, top_k=5):\n",
    "        \"\"\"Perform word analogy: word1 is to word2 as word3 is to ?\"\"\"\n",
    "        try:\n",
    "            if not all(w in word_to_index for w in [word1, word2, word3]):\n",
    "                missing = [w for w in [word1, word2, word3] if w not in word_to_index]\n",
    "                return f\"Words not in vocabulary: {missing}\"\n",
    "            \n",
    "            # Get embeddings\n",
    "            v1 = embeddings[word_to_index[word1]]\n",
    "            v2 = embeddings[word_to_index[word2]]\n",
    "            v3 = embeddings[word_to_index[word3]]\n",
    "            \n",
    "            # Calculate: word2 - word1 + word3\n",
    "            target_vector = v2 - v1 + v3\n",
    "            \n",
    "            # Find most similar words\n",
    "            similarities = np.dot(embeddings, target_vector) / (\n",
    "                np.linalg.norm(embeddings, axis=1) * np.linalg.norm(target_vector)\n",
    "            )\n",
    "            \n",
    "            # Get top candidates (excluding input words)\n",
    "            exclude_indices = {word_to_index[w] for w in [word1, word2, word3]}\n",
    "            candidates = []\n",
    "            \n",
    "            for idx in np.argsort(similarities)[::-1]:\n",
    "                if idx not in exclude_indices:\n",
    "                    candidates.append((index_to_word[idx], similarities[idx]))\n",
    "                    if len(candidates) >= top_k:\n",
    "                        break\n",
    "            \n",
    "            return candidates\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    print(f\"\\n🧠 WORD ANALOGIES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    analogy_tests = [\n",
    "        (\"king\", \"queen\", \"man\"),  # king is to queen as man is to ?\n",
    "        (\"big\", \"bigger\", \"small\"),  # big is to bigger as small is to ?\n",
    "        (\"good\", \"better\", \"bad\"),  # good is to better as bad is to ?\n",
    "    ]\n",
    "    \n",
    "    for word1, word2, word3 in analogy_tests:\n",
    "        result = word_analogy(word1, word2, word3, embeddings, word_to_index, index_to_word)\n",
    "        if isinstance(result, str):\n",
    "            print(f\"  {word1}:{word2} :: {word3}:? -> {result}\")\n",
    "        else:\n",
    "            candidates = ', '.join([f\"{w}({s:.3f})\" for w, s in result[:3]])\n",
    "            print(f\"  {word1}:{word2} :: {word3}:? -> {candidates}\")\n",
    "    \n",
    "    print(f\"\\n✅ Analysis complete! Model ready for saving and visualization.\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No best config found. Run the sweep first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 6**: Save Best Model and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 SAVE BEST MODEL AND EMBEDDINGS FOR FUTURE USE\n",
    "if best_config and best_model and 'embeddings' in locals():\n",
    "    print(\"💾 Saving best Word2Vec model and embeddings...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    import pickle\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Define the timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Create a directory for saved models\n",
    "    save_dir = os.path.join(\"../artifacts/word2vec_trained_model\", f\"best_run_{timestamp}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Save the embeddings as numpy array\n",
    "    embeddings_path = os.path.join(save_dir, \"word2vec_embeddings.npy\")\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    print(f\"✅ Saved embeddings: {embeddings_path}\")\n",
    "    print(f\"   Shape: {embeddings.shape}\")\n",
    "\n",
    "    # 2. Save vocabulary mappings\n",
    "    vocab_path = os.path.join(save_dir, \"word2vec_vocab.pkl\")\n",
    "    vocab_data = {\n",
    "        'word_to_index': word_to_index,\n",
    "        'index_to_word': index_to_word,\n",
    "        'vocabulary': vocabulary,\n",
    "        'vocab_size': len(vocabulary)\n",
    "    }\n",
    "\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(vocab_data, f)\n",
    "    print(f\"✅ Saved vocabulary: {vocab_path}\")\n",
    "    print(f\"   Vocabulary size: {len(vocabulary):,} words\")\n",
    "\n",
    "    # 3. Save the full model state\n",
    "    model_path = os.path.join(save_dir, \"word2vec_model.pth\")\n",
    "    model_save_data = {\n",
    "        'model_state_dict': best_model.state_dict(),\n",
    "        'model_config': {\n",
    "            'vocab_size': len(vocabulary),\n",
    "            'embedding_dim': best_config['embedding_dim'],\n",
    "            'architecture': 'SimpleWord2Vec_CBOW'\n",
    "        },\n",
    "        'training_config': best_config,\n",
    "        'training_info': {\n",
    "            'final_loss': final_loss,\n",
    "            'total_epochs': total_epochs,\n",
    "            'training_examples': MAX_EXAMPLES,\n",
    "            'training_date': datetime.now().isoformat(),\n",
    "            'sweep_id': sweep_id,\n",
    "            'best_run_name': best_run.name\n",
    "        }\n",
    "    }\n",
    "\n",
    "    torch.save(model_save_data, model_path)\n",
    "    print(f\"✅ Saved model state: {model_path}\")\n",
    "\n",
    "    # 4. Save human-readable configuration\n",
    "    config_path = os.path.join(save_dir, \"model_info.json\")\n",
    "    model_info = {\n",
    "        \"model_name\": \"SimpleWord2Vec_CBOW_BestFromSweep\",\n",
    "        \"embedding_dimensions\": best_config['embedding_dim'],\n",
    "        \"vocabulary_size\": len(vocabulary),\n",
    "        \"training_examples\": MAX_EXAMPLES,\n",
    "        \"final_loss\": final_loss,\n",
    "        \"training_epochs\": total_epochs,\n",
    "        \"sweep_id\": sweep_id,\n",
    "        \"best_run_name\": best_run.name,\n",
    "        \"best_hyperparameters\": {\n",
    "            \"learning_rate\": best_config['learning_rate'],\n",
    "            \"batch_size\": best_config['batch_size'],\n",
    "            \"window_size\": best_config['window_size'],\n",
    "            \"optimizer\": best_config['optimizer'],\n",
    "            \"weight_decay\": best_config['weight_decay']\n",
    "        },\n",
    "        \"dataset\": \"text8\",\n",
    "        \"training_date\": datetime.now().isoformat(),\n",
    "        \"usage_instructions\": {\n",
    "            \"load_embeddings\": \"embeddings = np.load('word2vec_embeddings.npy')\",\n",
    "            \"load_vocab\": \"with open('word2vec_vocab.pkl', 'rb') as f: vocab = pickle.load(f)\",\n",
    "            \"load_model\": \"checkpoint = torch.load('word2vec_model.pth')\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    print(f\"✅ Saved model info: {config_path}\")\n",
    "    \n",
    "    # Save the save directory path for visualization\n",
    "    SAVED_MODEL_DIR = save_dir\n",
    "    print(f\"\\n🎯 Model saved to: {SAVED_MODEL_DIR}\")\n",
    "    print(\"   This path will be used for visualization in the next step.\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No model to save. Make sure the analysis step completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 7**: Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎨 VISUALIZE WORD EMBEDDINGS FROM BEST MODEL\n",
    "if 'SAVED_MODEL_DIR' in locals():\n",
    "    print(\"🎨 Creating visualizations of word embeddings...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # Load embeddings and vocabulary from saved model\n",
    "    print(f\"📂 Loading from: {SAVED_MODEL_DIR}\")\n",
    "    \n",
    "    # Load embeddings\n",
    "    viz_embeddings_path = os.path.join(SAVED_MODEL_DIR, \"word2vec_embeddings.npy\")\n",
    "    viz_embeddings_all = np.load(viz_embeddings_path)\n",
    "    print(f\"✅ Loaded embeddings: {viz_embeddings_all.shape}\")\n",
    "    \n",
    "    # Load vocabulary\n",
    "    viz_vocab_path = os.path.join(SAVED_MODEL_DIR, \"word2vec_vocab.pkl\")\n",
    "    with open(viz_vocab_path, 'rb') as f:\n",
    "        viz_vocab_data = pickle.load(f)\n",
    "    \n",
    "    viz_word_to_index = viz_vocab_data['word_to_index']\n",
    "    viz_index_to_word = viz_vocab_data['index_to_word']\n",
    "    print(f\"✅ Loaded vocabulary: {len(viz_word_to_index):,} words\")\n",
    "    \n",
    "    # Words to visualize\n",
    "    viz_words = [\n",
    "        'dog', 'cat', 'bird', 'fish', 'horse', 'cow', 'pig',\n",
    "        'red', 'blue', 'green', 'yellow', 'black', 'white',\n",
    "        'one', 'two', 'three', 'four', 'five', 'six',\n",
    "        'america', 'england', 'france', 'china', 'city', 'country',\n",
    "        'computer', 'internet', 'digital', 'software', 'technology',\n",
    "        'happy', 'sad', 'love', 'angry', 'fear', 'joy'\n",
    "    ]\n",
    "\n",
    "    # Word categories\n",
    "    word_categories = {\n",
    "        'Animals': ['dog', 'cat', 'bird', 'fish', 'horse', 'cow', 'pig'],\n",
    "        'Colors': ['red', 'blue', 'green', 'yellow', 'black', 'white'],\n",
    "        'Numbers': ['one', 'two', 'three', 'four', 'five', 'six'],\n",
    "        'Places': ['america', 'england', 'france', 'china', 'city', 'country'],\n",
    "        'Tech': ['computer', 'internet', 'digital', 'software', 'technology'],\n",
    "        'Emotions': ['happy', 'sad', 'love', 'angry', 'fear', 'joy']\n",
    "    }\n",
    "\n",
    "    # Filter available words\n",
    "    available_words = [word for word in viz_words if word in viz_word_to_index]\n",
    "    print(f\"🔍 Found {len(available_words)} words in vocabulary for visualization\")\n",
    "    \n",
    "    if len(available_words) < 10:\n",
    "        print(\"❌ Not enough words for visualization\")\n",
    "    else:\n",
    "        # Get embeddings for available words\n",
    "        selected_embeddings = np.array([viz_embeddings_all[viz_word_to_index[word]] for word in available_words])\n",
    "        print(f\"📊 Visualization embeddings shape: {selected_embeddings.shape}\")\n",
    "\n",
    "        # 3D t-SNE\n",
    "        print(\"🔄 Computing 3D t-SNE...\")\n",
    "        tsne = TSNE(n_components=3, random_state=42, perplexity=min(30, len(available_words)-1))\n",
    "        embeddings_3d = tsne.fit_transform(selected_embeddings)\n",
    "\n",
    "        # Create 3D t-SNE plot\n",
    "        fig = plt.figure(figsize=(15, 12))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Color scheme\n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
    "        \n",
    "        # Plot points\n",
    "        for i, word in enumerate(available_words):\n",
    "            x, y, z = embeddings_3d[i]\n",
    "            \n",
    "            # Find category color\n",
    "            color = 'black'\n",
    "            for cat_name, cat_words in word_categories.items():\n",
    "                if word in cat_words:\n",
    "                    cat_index = list(word_categories.keys()).index(cat_name)\n",
    "                    color = colors[cat_index % len(colors)]\n",
    "                    break\n",
    "            \n",
    "            ax.scatter(x, y, z, c=color, s=100, alpha=0.7)\n",
    "            ax.text(x, y, z, word, fontsize=9)\n",
    "\n",
    "        ax.set_title('3D Word Embeddings (t-SNE) - Best Model', fontsize=16)\n",
    "        ax.set_xlabel('Component 1')\n",
    "        ax.set_ylabel('Component 2')\n",
    "        ax.set_zlabel('Component 3')\n",
    "\n",
    "        # Legend\n",
    "        legend_elements = []\n",
    "        for i, (cat_name, _) in enumerate(word_categories.items()):\n",
    "            color = colors[i % len(colors)]\n",
    "            legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                            markerfacecolor=color, markersize=8, label=cat_name))\n",
    "        ax.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save t-SNE plot\n",
    "        tsne_filename = f'{SAVED_MODEL_DIR}/embeddings_3d_tsne.png'\n",
    "        plt.savefig(tsne_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✅ Saved t-SNE visualization: {tsne_filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # 3D PCA\n",
    "        print(\"🔄 Computing 3D PCA...\")\n",
    "        pca = PCA(n_components=3)\n",
    "        embeddings_pca = pca.fit_transform(selected_embeddings)\n",
    "\n",
    "        # Create 3D PCA plot\n",
    "        fig2 = plt.figure(figsize=(15, 12))\n",
    "        ax2 = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Plot PCA points\n",
    "        for i, word in enumerate(available_words):\n",
    "            x, y, z = embeddings_pca[i]\n",
    "            \n",
    "            # Find category color\n",
    "            color = 'black'\n",
    "            for cat_name, cat_words in word_categories.items():\n",
    "                if word in cat_words:\n",
    "                    cat_index = list(word_categories.keys()).index(cat_name)\n",
    "                    color = colors[cat_index % len(colors)]\n",
    "                    break\n",
    "            \n",
    "            ax2.scatter(x, y, z, c=color, s=100, alpha=0.7)\n",
    "            ax2.text(x, y, z, word, fontsize=9)\n",
    "\n",
    "        ax2.set_title('3D Word Embeddings (PCA) - Best Model', fontsize=16)\n",
    "        ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "        ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "        ax2.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})')\n",
    "        ax2.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save PCA plot\n",
    "        pca_filename = f'{SAVED_MODEL_DIR}/embeddings_3d_pca.png'\n",
    "        plt.savefig(pca_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✅ Saved PCA visualization: {pca_filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"📈 PCA explained variance: {sum(pca.explained_variance_ratio_):.1%}\")\n",
    "        print(f\"\\n🎉 All visualizations complete!\")\n",
    "        print(f\"📁 Saved to model directory: {SAVED_MODEL_DIR}\")\n",
    "        print(f\"   - {os.path.basename(tsne_filename)}\")\n",
    "        print(f\"   - {os.path.basename(pca_filename)}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No saved model directory found. Please run the previous steps first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 8**: Fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 LOAD TRAINED MODEL AND CONTINUE TRAINING\n",
    "print(\"🔄 Loading trained Word2Vec model for continued training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# Path to saved model # Change this to your desired path\n",
    "FOLDER_TO_CONTINUE_TRAINING = \"artifacts/word2vec_trained_model/best_run_20250615_165915\"\n",
    "\n",
    "# 1. Load vocabulary\n",
    "print(\"📖 Loading vocabulary...\")\n",
    "vocab_path = os.path.join(FOLDER_TO_CONTINUE_TRAINING, \"word2vec_vocab.pkl\")\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "word_to_index = vocab_data['word_to_index']\n",
    "index_to_word = vocab_data['index_to_word']\n",
    "vocabulary = vocab_data['vocabulary']\n",
    "\n",
    "print(f\"✅ Loaded vocabulary: {len(vocabulary):,} words\")\n",
    "\n",
    "# 2. Load model checkpoint\n",
    "print(\"🔧 Loading model checkpoint...\")\n",
    "model_path = os.path.join(FOLDER_TO_CONTINUE_TRAINING, \"word2vec_model.pth\")\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "model_config = checkpoint['model_config']\n",
    "training_config = checkpoint['training_config']\n",
    "training_info = checkpoint['training_info']\n",
    "\n",
    "print(f\"✅ Loaded model checkpoint\")\n",
    "print(f\"   - Embedding dim: {model_config['embedding_dim']}\")\n",
    "print(f\"   - Vocab size: {model_config['vocab_size']:,}\")\n",
    "print(f\"   - Previous loss: {training_info['final_loss']:.4f}\")\n",
    "print(f\"   - Previous epochs: {training_info['total_epochs']}\")\n",
    "\n",
    "# 3. Recreate model architecture and load state\n",
    "print(\"🏗️  Recreating model architecture...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleWord2Vec(model_config['vocab_size'], model_config['embedding_dim']).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"✅ Model loaded on {device}\")\n",
    "\n",
    "# 4. Set up optimizer with same parameters\n",
    "print(\"⚙️  Setting up optimizer...\")\n",
    "if training_config['optimizer'] == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                          lr=training_config['learning_rate'], \n",
    "                          weight_decay=training_config['weight_decay'])\n",
    "elif training_config['optimizer'] == 'adamw':\n",
    "    optimizer = optim.AdamW(model.parameters(), \n",
    "                           lr=training_config['learning_rate'], \n",
    "                           weight_decay=training_config['weight_decay'])\n",
    "else:  # sgd\n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                         lr=training_config['learning_rate'],\n",
    "                         momentum=0.9, \n",
    "                         weight_decay=training_config['weight_decay'])\n",
    "\n",
    "print(f\"✅ Optimizer: {training_config['optimizer']} (lr={training_config['learning_rate']})\")\n",
    "\n",
    "# 5. Create training data with same parameters\n",
    "print(\"📊 Creating training data...\")\n",
    "contexts, targets = create_training_data(\n",
    "    indexed_words, \n",
    "    training_config['window_size'],\n",
    "    max_examples=MAX_EXAMPLES  # Same as before\n",
    ")\n",
    "\n",
    "dataset = TensorDataset(contexts, targets)\n",
    "dataloader = DataLoader(dataset, \n",
    "                       batch_size=training_config['batch_size'], \n",
    "                       shuffle=True)\n",
    "\n",
    "print(f\"✅ Training data ready: {len(contexts):,} examples\")\n",
    "print(f\"   - Window size: {training_config['window_size']}\")\n",
    "print(f\"   - Batch size: {training_config['batch_size']}\")\n",
    "\n",
    "print(\"🚀 Ready for continued training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 9**: Save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 SAVE BEST MODEL AND EMBEDDINGS FOR FUTURE USE\n",
    "if best_config and best_model and 'embeddings' in locals():\n",
    "    print(\"💾 Saving fine-tuned Word2Vec model and embeddings...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    import pickle\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Define the timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Create a directory for saved models\n",
    "    save_dir = os.path.join(\"../artifacts/word2vec_trained_model\", f\"fine_tuned_{timestamp}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Save the embeddings as numpy array\n",
    "    embeddings_path = os.path.join(save_dir, \"word2vec_embeddings.npy\")\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    print(f\"✅ Saved embeddings: {embeddings_path}\")\n",
    "    print(f\"   Shape: {embeddings.shape}\")\n",
    "\n",
    "    # 2. Save vocabulary mappings\n",
    "    vocab_path = os.path.join(save_dir, \"word2vec_vocab.pkl\")\n",
    "    vocab_data = {\n",
    "        'word_to_index': word_to_index,\n",
    "        'index_to_word': index_to_word,\n",
    "        'vocabulary': vocabulary,\n",
    "        'vocab_size': len(vocabulary)\n",
    "    }\n",
    "\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(vocab_data, f)\n",
    "    print(f\"✅ Saved vocabulary: {vocab_path}\")\n",
    "    print(f\"   Vocabulary size: {len(vocabulary):,} words\")\n",
    "\n",
    "    # 3. Save the full model state\n",
    "    model_path = os.path.join(save_dir, \"word2vec_model.pth\")\n",
    "    model_save_data = {\n",
    "        'model_state_dict': best_model.state_dict(),\n",
    "        'model_config': {\n",
    "            'vocab_size': len(vocabulary),\n",
    "            'embedding_dim': best_config['embedding_dim'],\n",
    "            'architecture': 'SimpleWord2Vec_CBOW'\n",
    "        },\n",
    "        'training_config': best_config,\n",
    "        'training_info': {\n",
    "            'final_loss': final_loss,\n",
    "            'total_epochs': total_epochs,\n",
    "            'training_examples': MAX_EXAMPLES,\n",
    "            'training_date': datetime.now().isoformat(),\n",
    "            'sweep_id': sweep_id,\n",
    "            'best_run_name': best_run.name\n",
    "        }\n",
    "    }\n",
    "\n",
    "    torch.save(model_save_data, model_path)\n",
    "    print(f\"✅ Saved model state: {model_path}\")\n",
    "\n",
    "    # 4. Save human-readable configuration\n",
    "    config_path = os.path.join(save_dir, \"model_info.json\")\n",
    "    model_info = {\n",
    "        \"model_name\": \"SimpleWord2Vec_CBOW_BestFromSweep\",\n",
    "        \"embedding_dimensions\": best_config['embedding_dim'],\n",
    "        \"vocabulary_size\": len(vocabulary),\n",
    "        \"training_examples\": MAX_EXAMPLES,\n",
    "        \"final_loss\": final_loss,\n",
    "        \"training_epochs\": total_epochs,\n",
    "        \"sweep_id\": sweep_id,\n",
    "        \"best_run_name\": best_run.name,\n",
    "        \"best_hyperparameters\": {\n",
    "            \"learning_rate\": best_config['learning_rate'],\n",
    "            \"batch_size\": best_config['batch_size'],\n",
    "            \"window_size\": best_config['window_size'],\n",
    "            \"optimizer\": best_config['optimizer'],\n",
    "            \"weight_decay\": best_config['weight_decay']\n",
    "        },\n",
    "        \"dataset\": \"text8\",\n",
    "        \"training_date\": datetime.now().isoformat(),\n",
    "        \"usage_instructions\": {\n",
    "            \"load_embeddings\": \"embeddings = np.load('word2vec_embeddings.npy')\",\n",
    "            \"load_vocab\": \"with open('word2vec_vocab.pkl', 'rb') as f: vocab = pickle.load(f)\",\n",
    "            \"load_model\": \"checkpoint = torch.load('word2vec_model.pth')\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    print(f\"✅ Saved model info: {config_path}\")\n",
    "    \n",
    "    # Save the save directory path for visualization\n",
    "    SAVED_MODEL_DIR = save_dir\n",
    "    print(f\"\\n🎯 Model saved to: {SAVED_MODEL_DIR}\")\n",
    "    print(\"   This path will be used for visualization in the next step.\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No model to save. Make sure the analysis step completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
