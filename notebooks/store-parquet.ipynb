{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the already combined dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the final combined dataset\n",
    "final_df = pd.read_parquet('workspace/data/hackernews_full_data.parquet')\n",
    "\n",
    "print(f\"Loaded dataset: {len(final_df):,} rows\")\n",
    "print(f\"Columns: {list(final_df.columns)}\")\n",
    "print(f\"Memory usage: {final_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"Date range: {final_df['time'].min()} to {final_df['time'].max()}\")\n",
    "print(f\"Score range: {final_df['score'].min()} to {final_df['score'].max()}\")\n",
    "\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Import libraries\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database successfully!\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Connect to database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=os.getenv('DB_NAME'),\n",
    "    user=os.getenv('DB_USER'),\n",
    "    password=os.getenv('DB_PASSWORD'),\n",
    "    host=os.getenv('DB_HOST'),\n",
    "    port=os.getenv('DB_PORT')\n",
    ")\n",
    "print(\"Connected to database successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8998/276969155.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  total_rows = pd.read_sql(count_query, conn).iloc[0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to process: 4,902,536\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Get total count first\n",
    "count_query = \"\"\"\n",
    "SELECT  COUNT(*) \n",
    "FROM \"hacker_news\".\"items\" \n",
    "WHERE score IS NOT NULL \n",
    "  AND title IS NOT NULL \n",
    "  AND by IS NOT NULL\n",
    "  AND type = 'story'\n",
    "\"\"\"\n",
    "\n",
    "total_rows = pd.read_sql(count_query, conn).iloc[0, 0]\n",
    "print(f\"Total rows to process: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process in 99 chunks of 50,000 rows each\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Set chunk parameters\n",
    "chunk_size = 50000  # Adjust this if needed\n",
    "num_chunks = (total_rows // chunk_size) + (1 if total_rows % chunk_size > 0 else 0)\n",
    "print(f\"Will process in {num_chunks} chunks of {chunk_size:,} rows each\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chunked loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8998/2324593453.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  chunk_df = pd.read_sql(chunk_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First chunk loaded: 50,000 rows\n",
      "Memory usage: 15.0 MB\n",
      "First chunk saved as workspace/data/chunks/chunk_0.parquet\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "print(\"Starting chunked loading...\")\n",
    "\n",
    "# CELL 6: Load first chunk (test)\n",
    "chunk_query = f\"\"\"\n",
    "SELECT id, by, time, url, score, title, descendants\n",
    "FROM \"hacker_news\".\"items\" \n",
    "WHERE score IS NOT NULL \n",
    "  AND title IS NOT NULL \n",
    "  AND by IS NOT NULL\n",
    "  AND type = 'story'\n",
    "ORDER BY time\n",
    "LIMIT {chunk_size} OFFSET 0\n",
    "\"\"\"\n",
    "\n",
    "chunk_df = pd.read_sql(chunk_query, conn)\n",
    "print(f\"First chunk loaded: {len(chunk_df):,} rows\")\n",
    "print(f\"Memory usage: {chunk_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Save first chunk\n",
    "chunk_filename = 'workspace/data/chunks/chunk_0.parquet'\n",
    "chunk_df.to_parquet(chunk_filename, index=False)\n",
    "all_chunks.append(chunk_filename)  # Store filename instead of dataframe\n",
    "print(f\"First chunk saved as {chunk_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8998/3387744160.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  chunk_df = pd.read_sql(chunk_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 2/99 saved: 50,000 rows -> workspace/data/chunks/chunk_1.parquet\n",
      "Chunk 3/99 saved: 50,000 rows -> workspace/data/chunks/chunk_2.parquet\n",
      "Chunk 4/99 saved: 50,000 rows -> workspace/data/chunks/chunk_3.parquet\n",
      "Chunk 5/99 saved: 50,000 rows -> workspace/data/chunks/chunk_4.parquet\n",
      "Chunk 6/99 saved: 50,000 rows -> workspace/data/chunks/chunk_5.parquet\n",
      "Chunk 7/99 saved: 50,000 rows -> workspace/data/chunks/chunk_6.parquet\n",
      "Chunk 8/99 saved: 50,000 rows -> workspace/data/chunks/chunk_7.parquet\n",
      "Chunk 9/99 saved: 50,000 rows -> workspace/data/chunks/chunk_8.parquet\n",
      "Chunk 10/99 saved: 50,000 rows -> workspace/data/chunks/chunk_9.parquet\n",
      "Chunk 11/99 saved: 50,000 rows -> workspace/data/chunks/chunk_10.parquet\n",
      "Chunk 12/99 saved: 50,000 rows -> workspace/data/chunks/chunk_11.parquet\n",
      "Chunk 13/99 saved: 50,000 rows -> workspace/data/chunks/chunk_12.parquet\n",
      "Chunk 14/99 saved: 50,000 rows -> workspace/data/chunks/chunk_13.parquet\n",
      "Chunk 15/99 saved: 50,000 rows -> workspace/data/chunks/chunk_14.parquet\n",
      "Chunk 16/99 saved: 50,000 rows -> workspace/data/chunks/chunk_15.parquet\n",
      "Chunk 17/99 saved: 50,000 rows -> workspace/data/chunks/chunk_16.parquet\n",
      "Chunk 18/99 saved: 50,000 rows -> workspace/data/chunks/chunk_17.parquet\n",
      "Chunk 19/99 saved: 50,000 rows -> workspace/data/chunks/chunk_18.parquet\n",
      "Chunk 20/99 saved: 50,000 rows -> workspace/data/chunks/chunk_19.parquet\n",
      "Chunk 21/99 saved: 50,000 rows -> workspace/data/chunks/chunk_20.parquet\n",
      "Chunk 22/99 saved: 50,000 rows -> workspace/data/chunks/chunk_21.parquet\n",
      "Chunk 23/99 saved: 50,000 rows -> workspace/data/chunks/chunk_22.parquet\n",
      "Chunk 24/99 saved: 50,000 rows -> workspace/data/chunks/chunk_23.parquet\n",
      "Chunk 25/99 saved: 50,000 rows -> workspace/data/chunks/chunk_24.parquet\n",
      "Chunk 26/99 saved: 50,000 rows -> workspace/data/chunks/chunk_25.parquet\n",
      "Chunk 27/99 saved: 50,000 rows -> workspace/data/chunks/chunk_26.parquet\n",
      "Chunk 28/99 saved: 50,000 rows -> workspace/data/chunks/chunk_27.parquet\n",
      "Chunk 29/99 saved: 50,000 rows -> workspace/data/chunks/chunk_28.parquet\n",
      "Chunk 30/99 saved: 50,000 rows -> workspace/data/chunks/chunk_29.parquet\n",
      "Chunk 31/99 saved: 50,000 rows -> workspace/data/chunks/chunk_30.parquet\n",
      "Chunk 32/99 saved: 50,000 rows -> workspace/data/chunks/chunk_31.parquet\n",
      "Chunk 33/99 saved: 50,000 rows -> workspace/data/chunks/chunk_32.parquet\n",
      "Chunk 34/99 saved: 50,000 rows -> workspace/data/chunks/chunk_33.parquet\n",
      "Chunk 35/99 saved: 50,000 rows -> workspace/data/chunks/chunk_34.parquet\n",
      "Chunk 36/99 saved: 50,000 rows -> workspace/data/chunks/chunk_35.parquet\n",
      "Chunk 37/99 saved: 50,000 rows -> workspace/data/chunks/chunk_36.parquet\n",
      "Chunk 38/99 saved: 50,000 rows -> workspace/data/chunks/chunk_37.parquet\n",
      "Chunk 39/99 saved: 50,000 rows -> workspace/data/chunks/chunk_38.parquet\n",
      "Chunk 40/99 saved: 50,000 rows -> workspace/data/chunks/chunk_39.parquet\n",
      "Chunk 41/99 saved: 50,000 rows -> workspace/data/chunks/chunk_40.parquet\n",
      "Chunk 42/99 saved: 50,000 rows -> workspace/data/chunks/chunk_41.parquet\n",
      "Chunk 43/99 saved: 50,000 rows -> workspace/data/chunks/chunk_42.parquet\n",
      "Chunk 44/99 saved: 50,000 rows -> workspace/data/chunks/chunk_43.parquet\n",
      "Chunk 45/99 saved: 50,000 rows -> workspace/data/chunks/chunk_44.parquet\n",
      "Chunk 46/99 saved: 50,000 rows -> workspace/data/chunks/chunk_45.parquet\n",
      "Chunk 47/99 saved: 50,000 rows -> workspace/data/chunks/chunk_46.parquet\n",
      "Chunk 48/99 saved: 50,000 rows -> workspace/data/chunks/chunk_47.parquet\n",
      "Chunk 49/99 saved: 50,000 rows -> workspace/data/chunks/chunk_48.parquet\n",
      "Chunk 50/99 saved: 50,000 rows -> workspace/data/chunks/chunk_49.parquet\n",
      "Chunk 51/99 saved: 50,000 rows -> workspace/data/chunks/chunk_50.parquet\n",
      "Chunk 52/99 saved: 50,000 rows -> workspace/data/chunks/chunk_51.parquet\n",
      "Chunk 53/99 saved: 50,000 rows -> workspace/data/chunks/chunk_52.parquet\n",
      "Chunk 54/99 saved: 50,000 rows -> workspace/data/chunks/chunk_53.parquet\n",
      "Chunk 55/99 saved: 50,000 rows -> workspace/data/chunks/chunk_54.parquet\n",
      "Chunk 56/99 saved: 50,000 rows -> workspace/data/chunks/chunk_55.parquet\n",
      "Chunk 57/99 saved: 50,000 rows -> workspace/data/chunks/chunk_56.parquet\n",
      "Chunk 58/99 saved: 50,000 rows -> workspace/data/chunks/chunk_57.parquet\n",
      "Chunk 59/99 saved: 50,000 rows -> workspace/data/chunks/chunk_58.parquet\n",
      "Chunk 60/99 saved: 50,000 rows -> workspace/data/chunks/chunk_59.parquet\n",
      "Chunk 61/99 saved: 50,000 rows -> workspace/data/chunks/chunk_60.parquet\n",
      "Chunk 62/99 saved: 50,000 rows -> workspace/data/chunks/chunk_61.parquet\n",
      "Chunk 63/99 saved: 50,000 rows -> workspace/data/chunks/chunk_62.parquet\n",
      "Chunk 64/99 saved: 50,000 rows -> workspace/data/chunks/chunk_63.parquet\n",
      "Chunk 65/99 saved: 50,000 rows -> workspace/data/chunks/chunk_64.parquet\n",
      "Chunk 66/99 saved: 50,000 rows -> workspace/data/chunks/chunk_65.parquet\n",
      "Chunk 67/99 saved: 50,000 rows -> workspace/data/chunks/chunk_66.parquet\n",
      "Chunk 68/99 saved: 50,000 rows -> workspace/data/chunks/chunk_67.parquet\n",
      "Chunk 69/99 saved: 50,000 rows -> workspace/data/chunks/chunk_68.parquet\n",
      "Chunk 70/99 saved: 50,000 rows -> workspace/data/chunks/chunk_69.parquet\n",
      "Chunk 71/99 saved: 50,000 rows -> workspace/data/chunks/chunk_70.parquet\n",
      "Chunk 72/99 saved: 50,000 rows -> workspace/data/chunks/chunk_71.parquet\n",
      "Chunk 73/99 saved: 50,000 rows -> workspace/data/chunks/chunk_72.parquet\n",
      "Chunk 74/99 saved: 50,000 rows -> workspace/data/chunks/chunk_73.parquet\n",
      "Chunk 75/99 saved: 50,000 rows -> workspace/data/chunks/chunk_74.parquet\n",
      "Chunk 76/99 saved: 50,000 rows -> workspace/data/chunks/chunk_75.parquet\n",
      "Chunk 77/99 saved: 50,000 rows -> workspace/data/chunks/chunk_76.parquet\n",
      "Chunk 78/99 saved: 50,000 rows -> workspace/data/chunks/chunk_77.parquet\n",
      "Chunk 79/99 saved: 50,000 rows -> workspace/data/chunks/chunk_78.parquet\n",
      "Chunk 80/99 saved: 50,000 rows -> workspace/data/chunks/chunk_79.parquet\n",
      "Chunk 81/99 saved: 50,000 rows -> workspace/data/chunks/chunk_80.parquet\n",
      "Chunk 82/99 saved: 50,000 rows -> workspace/data/chunks/chunk_81.parquet\n",
      "Chunk 83/99 saved: 50,000 rows -> workspace/data/chunks/chunk_82.parquet\n",
      "Chunk 84/99 saved: 50,000 rows -> workspace/data/chunks/chunk_83.parquet\n",
      "Chunk 85/99 saved: 50,000 rows -> workspace/data/chunks/chunk_84.parquet\n",
      "Chunk 86/99 saved: 50,000 rows -> workspace/data/chunks/chunk_85.parquet\n",
      "Chunk 87/99 saved: 50,000 rows -> workspace/data/chunks/chunk_86.parquet\n",
      "Chunk 88/99 saved: 50,000 rows -> workspace/data/chunks/chunk_87.parquet\n",
      "Chunk 89/99 saved: 50,000 rows -> workspace/data/chunks/chunk_88.parquet\n",
      "Chunk 90/99 saved: 50,000 rows -> workspace/data/chunks/chunk_89.parquet\n",
      "Chunk 91/99 saved: 50,000 rows -> workspace/data/chunks/chunk_90.parquet\n",
      "Chunk 92/99 saved: 50,000 rows -> workspace/data/chunks/chunk_91.parquet\n",
      "Chunk 93/99 saved: 50,000 rows -> workspace/data/chunks/chunk_92.parquet\n",
      "Chunk 94/99 saved: 50,000 rows -> workspace/data/chunks/chunk_93.parquet\n",
      "Chunk 95/99 saved: 50,000 rows -> workspace/data/chunks/chunk_94.parquet\n",
      "Chunk 96/99 saved: 50,000 rows -> workspace/data/chunks/chunk_95.parquet\n",
      "Chunk 97/99 saved: 50,000 rows -> workspace/data/chunks/chunk_96.parquet\n",
      "Chunk 98/99 saved: 50,000 rows -> workspace/data/chunks/chunk_97.parquet\n",
      "Chunk 99/99 saved: 2,536 rows -> workspace/data/chunks/chunk_98.parquet\n",
      "Completed loading 99 chunks\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Load remaining chunks in loop\n",
    "for i in range(1, num_chunks):\n",
    "    offset = i * chunk_size\n",
    "    \n",
    "    chunk_query = f\"\"\"\n",
    "    SELECT id, by, time, url, score, title, descendants\n",
    "    FROM \"hacker_news\".\"items\" \n",
    "    WHERE score IS NOT NULL \n",
    "      AND title IS NOT NULL \n",
    "      AND by IS NOT NULL\n",
    "      AND type = 'story'\n",
    "    ORDER BY time\n",
    "    LIMIT {chunk_size} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        chunk_df = pd.read_sql(chunk_query, conn)\n",
    "        if len(chunk_df) > 0:\n",
    "            # Save chunk immediately\n",
    "            chunk_filename = f'workspace/data/chunks/chunk_{i}.parquet'\n",
    "            chunk_df.to_parquet(chunk_filename, index=False)\n",
    "            all_chunks.append(chunk_filename)\n",
    "            print(f\"Chunk {i+1}/{num_chunks} saved: {len(chunk_df):,} rows -> {chunk_filename}\")\n",
    "        else:\n",
    "            print(f\"Chunk {i+1} was empty, stopping\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading chunk {i+1}: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"Completed loading {len(all_chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding all chunk files...\n",
      "Found 99 chunk files\n",
      "\n",
      "Combining all chunks...\n",
      "Loaded workspace/data/chunks/chunk_0.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_1.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_10.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_11.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_12.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_13.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_14.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_15.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_16.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_17.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_18.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_19.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_2.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_20.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_21.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_22.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_23.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_24.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_25.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_26.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_27.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_28.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_29.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_3.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_30.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_31.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_32.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_33.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_34.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_35.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_36.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_37.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_38.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_39.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_4.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_40.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_41.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_42.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_43.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_44.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_45.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_46.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_47.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_48.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_49.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_5.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_50.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_51.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_52.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_53.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_54.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_55.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_56.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_57.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_58.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_59.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_6.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_60.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_61.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_62.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_63.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_64.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_65.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_66.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_67.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_68.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_69.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_7.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_70.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_71.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_72.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_73.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_74.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_75.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_76.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_77.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_78.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_79.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_8.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_80.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_81.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_82.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_83.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_84.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_85.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_86.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_87.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_88.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_89.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_9.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_90.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_91.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_92.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_93.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_94.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_95.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_96.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_97.parquet: 50,000 rows\n",
      "Loaded workspace/data/chunks/chunk_98.parquet: 2,536 rows\n",
      "\n",
      "Final combined dataset: 4,902,536 rows\n",
      "\n",
      "Final dataset saved as workspace/data/hackernews_full_data.parquet\n",
      "\n",
      "Cleaning up chunk files...\n",
      "Removed workspace/data/chunks/chunk_0.parquet\n",
      "Removed workspace/data/chunks/chunk_1.parquet\n",
      "Removed workspace/data/chunks/chunk_10.parquet\n",
      "Removed workspace/data/chunks/chunk_11.parquet\n",
      "Removed workspace/data/chunks/chunk_12.parquet\n",
      "Removed workspace/data/chunks/chunk_13.parquet\n",
      "Removed workspace/data/chunks/chunk_14.parquet\n",
      "Removed workspace/data/chunks/chunk_15.parquet\n",
      "Removed workspace/data/chunks/chunk_16.parquet\n",
      "Removed workspace/data/chunks/chunk_17.parquet\n",
      "Removed workspace/data/chunks/chunk_18.parquet\n",
      "Removed workspace/data/chunks/chunk_19.parquet\n",
      "Removed workspace/data/chunks/chunk_2.parquet\n",
      "Removed workspace/data/chunks/chunk_20.parquet\n",
      "Removed workspace/data/chunks/chunk_21.parquet\n",
      "Removed workspace/data/chunks/chunk_22.parquet\n",
      "Removed workspace/data/chunks/chunk_23.parquet\n",
      "Removed workspace/data/chunks/chunk_24.parquet\n",
      "Removed workspace/data/chunks/chunk_25.parquet\n",
      "Removed workspace/data/chunks/chunk_26.parquet\n",
      "Removed workspace/data/chunks/chunk_27.parquet\n",
      "Removed workspace/data/chunks/chunk_28.parquet\n",
      "Removed workspace/data/chunks/chunk_29.parquet\n",
      "Removed workspace/data/chunks/chunk_3.parquet\n",
      "Removed workspace/data/chunks/chunk_30.parquet\n",
      "Removed workspace/data/chunks/chunk_31.parquet\n",
      "Removed workspace/data/chunks/chunk_32.parquet\n",
      "Removed workspace/data/chunks/chunk_33.parquet\n",
      "Removed workspace/data/chunks/chunk_34.parquet\n",
      "Removed workspace/data/chunks/chunk_35.parquet\n",
      "Removed workspace/data/chunks/chunk_36.parquet\n",
      "Removed workspace/data/chunks/chunk_37.parquet\n",
      "Removed workspace/data/chunks/chunk_38.parquet\n",
      "Removed workspace/data/chunks/chunk_39.parquet\n",
      "Removed workspace/data/chunks/chunk_4.parquet\n",
      "Removed workspace/data/chunks/chunk_40.parquet\n",
      "Removed workspace/data/chunks/chunk_41.parquet\n",
      "Removed workspace/data/chunks/chunk_42.parquet\n",
      "Removed workspace/data/chunks/chunk_43.parquet\n",
      "Removed workspace/data/chunks/chunk_44.parquet\n",
      "Removed workspace/data/chunks/chunk_45.parquet\n",
      "Removed workspace/data/chunks/chunk_46.parquet\n",
      "Removed workspace/data/chunks/chunk_47.parquet\n",
      "Removed workspace/data/chunks/chunk_48.parquet\n",
      "Removed workspace/data/chunks/chunk_49.parquet\n",
      "Removed workspace/data/chunks/chunk_5.parquet\n",
      "Removed workspace/data/chunks/chunk_50.parquet\n",
      "Removed workspace/data/chunks/chunk_51.parquet\n",
      "Removed workspace/data/chunks/chunk_52.parquet\n",
      "Removed workspace/data/chunks/chunk_53.parquet\n",
      "Removed workspace/data/chunks/chunk_54.parquet\n",
      "Removed workspace/data/chunks/chunk_55.parquet\n",
      "Removed workspace/data/chunks/chunk_56.parquet\n",
      "Removed workspace/data/chunks/chunk_57.parquet\n",
      "Removed workspace/data/chunks/chunk_58.parquet\n",
      "Removed workspace/data/chunks/chunk_59.parquet\n",
      "Removed workspace/data/chunks/chunk_6.parquet\n",
      "Removed workspace/data/chunks/chunk_60.parquet\n",
      "Removed workspace/data/chunks/chunk_61.parquet\n",
      "Removed workspace/data/chunks/chunk_62.parquet\n",
      "Removed workspace/data/chunks/chunk_63.parquet\n",
      "Removed workspace/data/chunks/chunk_64.parquet\n",
      "Removed workspace/data/chunks/chunk_65.parquet\n",
      "Removed workspace/data/chunks/chunk_66.parquet\n",
      "Removed workspace/data/chunks/chunk_67.parquet\n",
      "Removed workspace/data/chunks/chunk_68.parquet\n",
      "Removed workspace/data/chunks/chunk_69.parquet\n",
      "Removed workspace/data/chunks/chunk_7.parquet\n",
      "Removed workspace/data/chunks/chunk_70.parquet\n",
      "Removed workspace/data/chunks/chunk_71.parquet\n",
      "Removed workspace/data/chunks/chunk_72.parquet\n",
      "Removed workspace/data/chunks/chunk_73.parquet\n",
      "Removed workspace/data/chunks/chunk_74.parquet\n",
      "Removed workspace/data/chunks/chunk_75.parquet\n",
      "Removed workspace/data/chunks/chunk_76.parquet\n",
      "Removed workspace/data/chunks/chunk_77.parquet\n",
      "Removed workspace/data/chunks/chunk_78.parquet\n",
      "Removed workspace/data/chunks/chunk_79.parquet\n",
      "Removed workspace/data/chunks/chunk_8.parquet\n",
      "Removed workspace/data/chunks/chunk_80.parquet\n",
      "Removed workspace/data/chunks/chunk_81.parquet\n",
      "Removed workspace/data/chunks/chunk_82.parquet\n",
      "Removed workspace/data/chunks/chunk_83.parquet\n",
      "Removed workspace/data/chunks/chunk_84.parquet\n",
      "Removed workspace/data/chunks/chunk_85.parquet\n",
      "Removed workspace/data/chunks/chunk_86.parquet\n",
      "Removed workspace/data/chunks/chunk_87.parquet\n",
      "Removed workspace/data/chunks/chunk_88.parquet\n",
      "Removed workspace/data/chunks/chunk_89.parquet\n",
      "Removed workspace/data/chunks/chunk_9.parquet\n",
      "Removed workspace/data/chunks/chunk_90.parquet\n",
      "Removed workspace/data/chunks/chunk_91.parquet\n",
      "Removed workspace/data/chunks/chunk_92.parquet\n",
      "Removed workspace/data/chunks/chunk_93.parquet\n",
      "Removed workspace/data/chunks/chunk_94.parquet\n",
      "Removed workspace/data/chunks/chunk_95.parquet\n",
      "Removed workspace/data/chunks/chunk_96.parquet\n",
      "Removed workspace/data/chunks/chunk_97.parquet\n",
      "Removed workspace/data/chunks/chunk_98.parquet\n",
      "\n",
      "Process completed! ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"Finding all chunk files...\")\n",
    "chunk_files = sorted(glob.glob('workspace/data/chunks/chunk_*.parquet'))\n",
    "print(f\"Found {len(chunk_files)} chunk files\")\n",
    "\n",
    "print(\"\\nCombining all chunks...\")\n",
    "combined_chunks = []\n",
    "\n",
    "for chunk_file in chunk_files:\n",
    "    chunk_df = pd.read_parquet(chunk_file)\n",
    "    combined_chunks.append(chunk_df)\n",
    "    print(f\"Loaded {chunk_file}: {len(chunk_df):,} rows\")\n",
    "\n",
    "final_df = pd.concat(combined_chunks, ignore_index=True)\n",
    "print(f\"\\nFinal combined dataset: {len(final_df):,} rows\")\n",
    "\n",
    "# Save final dataset\n",
    "output_file = 'workspace/data/hackernews_full_data.parquet'\n",
    "final_df.to_parquet(output_file, index=False)\n",
    "print(f\"\\nFinal dataset saved as {output_file}\")\n",
    "\n",
    "# Clean up individual chunk files\n",
    "print(\"\\nCleaning up chunk files...\")\n",
    "for chunk_file in chunk_files:\n",
    "    try:\n",
    "        os.remove(chunk_file)\n",
    "        print(f\"Removed {chunk_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not remove {chunk_file}: {e}\")\n",
    "\n",
    "print(\"\\nProcess completed! ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Save final dataset and cleanup\n",
    "final_df.to_parquet('workspace/data/hackernews_full_data.parquet', index=False)\n",
    "print(\"Final dataset saved as hackernews_full_data.parquet\")\n",
    "\n",
    "# Clean up individual chunk files\n",
    "for chunk_file in all_chunks:\n",
    "    try:\n",
    "        os.remove(chunk_file)\n",
    "        print(f\"Removed {chunk_file}\")\n",
    "    except:\n",
    "        print(f\"Could not remove {chunk_file}\")\n",
    "\n",
    "print(\"Cleanup completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset info:\n",
      "Shape: (4902536, 7)\n",
      "Memory usage: 1605.0 MB\n",
      "Date range: 2006-10-09 19:21:51 to 2024-10-13 23:53:00\n",
      "Score range: -1 to 6015\n"
     ]
    }
   ],
   "source": [
    "# CELL 10: Verify final dataset\n",
    "print(\"Final dataset info:\")\n",
    "print(f\"Shape: {final_df.shape}\")\n",
    "print(f\"Memory usage: {final_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"Date range: {final_df['time'].min()} to {final_df['time'].max()}\")\n",
    "print(f\"Score range: {final_df['score'].min()} to {final_df['score'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Close database connection\n",
    "conn.close()\n",
    "print(\"Database connection closed\")\n",
    "print(\"Data loading complete! ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>by</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>descendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>pg</td>\n",
       "      <td>2006-10-09 19:21:51</td>\n",
       "      <td>http://ycombinator.com</td>\n",
       "      <td>57</td>\n",
       "      <td>Y Combinator</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>phyllis</td>\n",
       "      <td>2006-10-09 19:30:28</td>\n",
       "      <td>http://www.paulgraham.com/mit.html</td>\n",
       "      <td>16</td>\n",
       "      <td>A Student's Guide to Startups</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>phyllis</td>\n",
       "      <td>2006-10-09 19:40:33</td>\n",
       "      <td>http://www.foundersatwork.com/stevewozniak.html</td>\n",
       "      <td>7</td>\n",
       "      <td>Woz Interview: the early days of Apple</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>onebeerdave</td>\n",
       "      <td>2006-10-09 19:47:42</td>\n",
       "      <td>http://avc.blogs.com/a_vc/2006/10/the_nyc_deve...</td>\n",
       "      <td>5</td>\n",
       "      <td>NYC Developer Dilemma</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>perler</td>\n",
       "      <td>2006-10-09 19:51:04</td>\n",
       "      <td>http://www.techcrunch.com/2006/10/09/google-yo...</td>\n",
       "      <td>7</td>\n",
       "      <td>Google, YouTube acquisition announcement could...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           by                time  \\\n",
       "0   1           pg 2006-10-09 19:21:51   \n",
       "1   2      phyllis 2006-10-09 19:30:28   \n",
       "2   3      phyllis 2006-10-09 19:40:33   \n",
       "3   4  onebeerdave 2006-10-09 19:47:42   \n",
       "4   5       perler 2006-10-09 19:51:04   \n",
       "\n",
       "                                                 url  score  \\\n",
       "0                             http://ycombinator.com     57   \n",
       "1                 http://www.paulgraham.com/mit.html     16   \n",
       "2    http://www.foundersatwork.com/stevewozniak.html      7   \n",
       "3  http://avc.blogs.com/a_vc/2006/10/the_nyc_deve...      5   \n",
       "4  http://www.techcrunch.com/2006/10/09/google-yo...      7   \n",
       "\n",
       "                                               title  descendants  \n",
       "0                                       Y Combinator         15.0  \n",
       "1                      A Student's Guide to Startups          0.0  \n",
       "2             Woz Interview: the early days of Apple          0.0  \n",
       "3                              NYC Developer Dilemma          0.0  \n",
       "4  Google, YouTube acquisition announcement could...          0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL 12: Quick data preview\n",
    "print(\"Sample data:\")\n",
    "final_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SQL query to pandas operations\n",
    "# Original SQL filters and selects specific columns with calculated fields\n",
    "\n",
    "# Filter the data (note: type='story' and by IS NOT NULL already applied during data loading)\n",
    "filtered_df = final_df[\n",
    "    (final_df['score'] >= 0)  # score >= 0 filter\n",
    "].copy()\n",
    "\n",
    "# Add calculated columns\n",
    "filtered_df['title_length'] = filtered_df['title'].str.len()\n",
    "filtered_df['title_word_count'] = filtered_df['title'].str.strip().str.split().str.len()\n",
    "\n",
    "# Select desired columns and sort by time descending\n",
    "result_df = filtered_df[['id', 'score', 'title', 'title_length', 'title_word_count']].sort_values('time', ascending=False)\n",
    "\n",
    "print(f\"Filtered dataset: {len(result_df):,} rows\")\n",
    "print(\"\\nSample results:\")\n",
    "result_df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
